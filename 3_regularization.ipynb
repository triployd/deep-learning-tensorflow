{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (20000, 28, 28) (20000,)\n",
      "Validation set (1000, 28, 28) (1000,)\n",
      "Test set (1000, 28, 28) (1000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (20000, 784) (20000, 10)\n",
      "Validation set (1000, 784) (1000, 10)\n",
      "Test set (1000, 784) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 23.459045\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 500: 2.864659\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1000: 1.676971\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1500: 0.940992\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 2000: 0.818083\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.842206\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 3000: 0.684638\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # tf.initialize_all_variables().run()\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different values of regularization parameter:\n",
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSikfoklfKaV8yP8H/G59hv1bSggAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2421fd9d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying with 2 layer network:\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables to be trained:\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 624.919617\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 24.5%\n",
      "Minibatch loss at step 500: 189.312317\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1000: 114.554863\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1500: 69.480759\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 42.038792\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2500: 25.587362\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 3000: 15.578646\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzFKX/XpvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24222b22160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 340.435242\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 37.0%\n",
      "Minibatch loss at step 2: 1245.136230\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 42.5%\n",
      "Minibatch loss at step 4: 199.472717\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 57.7%\n",
      "Minibatch loss at step 6: 18.508236\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Test accuracy: 72.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 492.430634\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 2: 1312.690430\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 44.3%\n",
      "Minibatch loss at step 4: 158.316925\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 61.0%\n",
      "Minibatch loss at step 6: 13.265211\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 8: 9.740487\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 10: 0.237335\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 12: 3.833082\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 14: 0.578198\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 16: 0.024345\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 18: 3.281330\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 20: 0.508060\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 22: 1.081269\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 24: 0.458300\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 26: 1.741356\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 28: 0.193819\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 32: 0.436477\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 40: 1.348243\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 42: 0.444000\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 44: 0.104727\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 48: 0.245596\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 50: 1.383743\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 52: 0.011658\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 56: 0.360765\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 60: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 78: 0.366657\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 88: 1.401177\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Test accuracy: 77.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 layers with different approaches to initialize variables:\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.235336\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 35.3%\n",
      "Minibatch loss at step 500: 0.910618\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1000: 0.740940\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1500: 0.512508\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2000: 0.410669\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2500: 0.374799\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3000: 0.356399\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3500: 0.299779\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4000: 0.276345\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4500: 0.285173\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5000: 0.258334\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.265873\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6000: 0.232814\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6500: 0.243121\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 7000: 0.258534\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7500: 0.243240\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8000: 0.234261\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8500: 0.225414\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 9000: 0.254171\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 layer deeper with no dropout:\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304376\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 28.9%\n",
      "Minibatch loss at step 500: 0.270164\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1000: 0.207877\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1500: 0.047446\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 2000: 0.048519\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2500: 0.095116\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 0.040241\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3500: 0.052991\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 4000: 0.010560\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4500: 0.002757\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5000: 0.000758\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5500: 0.023671\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 6000: 0.000469\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6500: 0.013634\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 7000: 0.038657\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 7500: 0.016809\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 8000: 0.010220\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 8500: 0.014412\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9000: 0.043557\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9500: 0.009335\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 10000: 0.000310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 10500: 0.000217\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 11000: 0.003802\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 11500: 0.019611\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 12000: 0.000347\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 12500: 0.000413\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 13000: 0.000602\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 13500: 0.009175\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 14000: 0.012876\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 14500: 0.007399\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 15000: 0.000575\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 15500: 0.001484\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 16000: 0.000136\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 16500: 0.000423\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 17000: 0.034561\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 17500: 0.000522\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 18000: 0.000204\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 92.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add dropout:\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.893803\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 29.2%\n",
      "Minibatch loss at step 500: 0.562845\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 0.567501\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1500: 0.302266\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2000: 0.375327\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 2500: 0.315263\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 0.340623\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3500: 0.287551\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4000: 0.357902\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4500: 0.578391\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 5000: 0.389456\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 5500: 0.312291\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 6000: 0.252191\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6500: 0.204500\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7000: 0.142887\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7500: 0.240419\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 8000: 0.258691\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 8500: 0.227035\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 9000: 0.170073\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 9500: 0.305022\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 10000: 0.146508\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 10500: 0.113249\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 11000: 0.129857\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 11500: 0.173011\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 12000: 0.144520\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 12500: 0.177181\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 13000: 0.116726\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 13500: 0.205741\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 14000: 0.107462\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 14500: 0.122084\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 15000: 0.044402\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 15500: 0.034768\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 16000: 0.085805\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 16500: 0.069088\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 17000: 0.116611\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 17500: 0.056676\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 18000: 0.159403\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 18500: 0.096271\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 19000: 0.102054\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 19500: 0.024225\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 20000: 0.068711\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.1%\n",
      "Test accuracy: 93.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
